{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-25T10:42:37.604119Z","iopub.execute_input":"2022-03-25T10:42:37.604958Z","iopub.status.idle":"2022-03-25T10:42:37.635401Z","shell.execute_reply.started":"2022-03-25T10:42:37.604834Z","shell.execute_reply":"2022-03-25T10:42:37.634547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install tensorflow==2.3","metadata":{"execution":{"iopub.status.busy":"2022-03-22T14:15:29.912688Z","iopub.execute_input":"2022-03-22T14:15:29.912956Z","iopub.status.idle":"2022-03-22T14:16:36.163556Z","shell.execute_reply.started":"2022-03-22T14:15:29.912925Z","shell.execute_reply":"2022-03-22T14:16:36.162713Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\ntf.__version__","metadata":{"execution":{"iopub.status.busy":"2022-03-23T08:48:42.966036Z","iopub.execute_input":"2022-03-23T08:48:42.966293Z","iopub.status.idle":"2022-03-23T08:48:47.391877Z","shell.execute_reply.started":"2022-03-23T08:48:42.966265Z","shell.execute_reply":"2022-03-23T08:48:47.391141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport io\nimport cv2\nimport numpy as np\nfrom os import listdir\nfrom os.path import isfile, join\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn import svm\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\n\n#import keras\nimport tensorflow as tf\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.applications import ResNet50\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\nfrom skimage.feature import hog\nfrom skimage import data, exposure\n\nimport tensorflow_addons as tfa\nimport random\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2022-03-25T10:42:40.589575Z","iopub.execute_input":"2022-03-25T10:42:40.589848Z","iopub.status.idle":"2022-03-25T10:42:47.125362Z","shell.execute_reply.started":"2022-03-25T10:42:40.589817Z","shell.execute_reply":"2022-03-25T10:42:47.12462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install tensorflow_addons\n# !pip install keras_applications \n# !pip install keras_preprocessing \n# !pip install git+https://github.com/rcmalli/keras-vggface.git","metadata":{"execution":{"iopub.status.busy":"2022-03-23T08:48:54.9228Z","iopub.execute_input":"2022-03-23T08:48:54.923044Z","iopub.status.idle":"2022-03-23T08:49:30.442328Z","shell.execute_reply.started":"2022-03-23T08:48:54.923016Z","shell.execute_reply":"2022-03-23T08:49:30.441478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import backend as K\ndef preprocess_input(x, data_format=None, version=1):\n    x_temp = np.copy(x)\n    if data_format is None:\n        data_format = K.image_data_format()\n    assert data_format in {'channels_last', 'channels_first'}\n\n    if version == 1:\n        if data_format == 'channels_first':\n            x_temp = x_temp[:, ::-1, ...]\n            x_temp[:, 0, :, :] -= 93.5940\n            x_temp[:, 1, :, :] -= 104.7624\n            x_temp[:, 2, :, :] -= 129.1863\n        else:\n            x_temp = x_temp[..., ::-1]\n            x_temp[..., 0] -= 93.5940\n            x_temp[..., 1] -= 104.7624\n            x_temp[..., 2] -= 129.1863\n\n    elif version == 2:\n        if data_format == 'channels_first':\n            x_temp = x_temp[:, ::-1, ...]\n            x_temp[:, 0, :, :] -= 91.4953\n            x_temp[:, 1, :, :] -= 103.8827\n            x_temp[:, 2, :, :] -= 131.0912\n        else:\n            x_temp = x_temp[..., ::-1]\n            x_temp[..., 0] -= 91.4953\n            x_temp[..., 1] -= 103.8827\n            x_temp[..., 2] -= 131.0912\n    else:\n        raise NotImplementedError\n\n    return x_temp","metadata":{"execution":{"iopub.status.busy":"2022-03-25T10:42:47.126711Z","iopub.execute_input":"2022-03-25T10:42:47.126976Z","iopub.status.idle":"2022-03-25T10:42:47.138494Z","shell.execute_reply.started":"2022-03-25T10:42:47.126939Z","shell.execute_reply":"2022-03-25T10:42:47.137923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SiameseNetwork(tf.keras.Model):\n    def __init__(self, vgg_face):\n        super(SiameseNetwork, self).__init__()\n        self.vgg_face = vgg_face\n        \n    @tf.function\n    def call(self, inputs):\n        image_1, image_2, image_3 =  inputs\n        with tf.name_scope(\"Anchor\") as scope:\n            feature_1 = self.vgg_face(image_1)\n            feature_1 = tf.math.l2_normalize(feature_1, axis=-1)\n        with tf.name_scope(\"Positive\") as scope:\n            feature_2 = self.vgg_face(image_2)\n            feature_2 = tf.math.l2_normalize(feature_2, axis=-1)\n        with tf.name_scope(\"Negative\") as scope:\n            feature_3 = self.vgg_face(image_3)\n            feature_3 = tf.math.l2_normalize(feature_3, axis=-1)\n        return [feature_1, feature_2, feature_3]\n    \n    @tf.function\n    def get_features(self, inputs):\n        return tf.math.l2_normalize(self.vgg_face(inputs, training=False), axis=-1)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T10:42:50.621041Z","iopub.execute_input":"2022-03-25T10:42:50.623018Z","iopub.status.idle":"2022-03-25T10:42:50.640493Z","shell.execute_reply.started":"2022-03-25T10:42:50.622975Z","shell.execute_reply":"2022-03-25T10:42:50.639793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataGenerator(tf.keras.utils.Sequence):\n    def __init__(self, dataset_path, batch_size=5, shuffle=True):\n        self.dataset = self.curate_dataset(dataset_path)\n        self.dataset_path = dataset_path\n        self.shuffle = shuffle\n        self.batch_size =batch_size\n        self.no_of_people = len(list(self.dataset.keys()))\n        self.on_epoch_end()\n        print(self.dataset.keys())\n        \n    def __getitem__(self, index):\n        people = list(self.dataset.keys())[index * self.batch_size: (index + 1) * self.batch_size]\n        P = []\n        A = []\n        N = []\n        \n        for person in people:\n            anchor_index = random.randint(0, len(self.dataset[person])-1)\n            a = self.get_image(person, anchor_index)\n            \n            positive_index = random.randint(0, len(self.dataset[person])-1)\n            while positive_index == anchor_index and len(self.dataset[person]) != 1:\n                positive_index = random.randint(0, len(self.dataset[person])-1)\n            p = self.get_image(person, positive_index)\n            \n            negative_person_index = random.randint(0, self.no_of_people - 1)\n            negative_person = list(self.dataset.keys())[negative_person_index]\n            while negative_person == person:\n                negative_person_index = random.randint(0, self.no_of_people - 1)\n                negative_person = list(self.dataset.keys())[negative_person_index]\n            \n            negative_index = random.randint(0, len(self.dataset[negative_person])-1)\n            n = self.get_image(negative_person, negative_index)\n            P.append(p)\n            A.append(a)\n            N.append(n)\n        A = np.asarray(A)\n        N = np.asarray(N)\n        P = np.asarray(P)\n        return [A, P, N]\n        \n    def __len__(self):\n        return self.no_of_people // self.batch_size\n        \n    def curate_dataset(self, dataset_path):\n        dataset = {}\n        dirs = [dir for dir in listdir(dataset_path)]\n        for dir in dirs: \n            fichiers = [f for f in listdir(dataset_path+dir) if \"jpeg\" in f or \"png\" in f]\n            for f in fichiers:\n                if dir in dataset.keys():\n                    dataset[dir].append(f)\n                else:\n                    dataset[dir] = [f]\n        return dataset\n    \n    def on_epoch_end(self):\n        if self.shuffle:\n            keys = list(self.dataset.keys())\n            random.shuffle(keys)\n            dataset_ =  {}\n            for key in keys:\n                dataset_[key] = self.dataset[key]\n            self.dataset = dataset_\n            \n    def get_image(self, person, index):\n        img = cv2.imread(os.path.join(self.dataset_path, os.path.join(person, self.dataset[person][index])))\n        img = cv2.resize(img, (224, 224))\n        img = np.asarray(img, dtype=np.float64)\n        img = preprocess_input(img)\n        return img","metadata":{"execution":{"iopub.status.busy":"2022-03-25T10:42:51.923771Z","iopub.execute_input":"2022-03-25T10:42:51.92412Z","iopub.status.idle":"2022-03-25T10:42:51.941809Z","shell.execute_reply.started":"2022-03-25T10:42:51.924085Z","shell.execute_reply":"2022-03-25T10:42:51.940861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"K = tf.keras.backend\ndef loss_function(x, alpha = 0.2):\n    # Triplet Loss function.\n    anchor,positive,negative = x\n    # distance between the anchor and the positive\n    pos_dist = K.sum(K.square(anchor-positive),axis=1)\n    # distance between the anchor and the negative\n    neg_dist = K.sum(K.square(anchor-negative),axis=1)\n    # compute loss\n    basic_loss = pos_dist-neg_dist+alpha\n    loss = K.mean(K.maximum(basic_loss,0.0))\n    return loss","metadata":{"execution":{"iopub.status.busy":"2022-03-25T10:42:53.024958Z","iopub.execute_input":"2022-03-25T10:42:53.025498Z","iopub.status.idle":"2022-03-25T10:42:53.031788Z","shell.execute_reply.started":"2022-03-25T10:42:53.02546Z","shell.execute_reply":"2022-03-25T10:42:53.030937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import ZeroPadding2D, Convolution2D, MaxPooling2D, Dropout, Flatten, Activation\n\ndef vgg_face():\t\n    model = Sequential()\n    model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n    model.add(Convolution2D(64, (3, 3), activation='relu'))\n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2,2), strides=(2,2)))\n    \n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(128, (3, 3), activation='relu'))\n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(128, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2,2), strides=(2,2)))\n    \n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(256, (3, 3), activation='relu'))\n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(256, (3, 3), activation='relu'))\n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(256, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2,2), strides=(2,2)))\n    \n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(512, (3, 3), activation='relu'))\n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(512, (3, 3), activation='relu'))\n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(512, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2,2), strides=(2,2)))\n    \n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(512, (3, 3), activation='relu'))\n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(512, (3, 3), activation='relu'))\n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(512, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2,2), strides=(2,2)))\n    \n    model.add(Convolution2D(4096, (7, 7), activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Convolution2D(4096, (1, 1), activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Convolution2D(2622, (1, 1)))\n    model.add(Flatten())\n    model.add(Activation('softmax'))\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-03-25T10:42:53.968509Z","iopub.execute_input":"2022-03-25T10:42:53.969281Z","iopub.status.idle":"2022-03-25T10:42:53.986695Z","shell.execute_reply.started":"2022-03-25T10:42:53.969232Z","shell.execute_reply":"2022-03-25T10:42:53.985937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Knowledge distillation","metadata":{}},{"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(learning_rate=0.00006)\n#binary_cross_entropy = tf.keras.losses.BinaryCrossentropy()\ndef train(X):\n    with tf.GradientTape() as tape:\n        y_pred = model(X)\n        loss = loss_function(y_pred)\n    grad = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(grad, model.trainable_variables))\n    return loss","metadata":{"execution":{"iopub.status.busy":"2022-03-23T09:01:45.248655Z","iopub.execute_input":"2022-03-23T09:01:45.249179Z","iopub.status.idle":"2022-03-23T09:01:45.255117Z","shell.execute_reply.started":"2022-03-23T09:01:45.24914Z","shell.execute_reply":"2022-03-23T09:01:45.254355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_generator = DataGenerator(dataset_path='../input/dataset4/dataset3/train/', batch_size=5)\n\n\nlosses = []\naccuracy = []\nepochs = 5\nno_of_batches = data_generator.__len__()\nfor i in range(1, epochs+1, 1):\n    loss = 0\n    with tqdm(total=no_of_batches) as pbar:\n        \n        description = \"Epoch \" + str(i) + \"/\" + str(epochs)\n        pbar.set_description_str(description)\n        \n        for j in range(no_of_batches):\n            data = data_generator[j]\n            temp = train(data)\n            loss += temp\n            \n            pbar.update()\n            print_statement = \"Loss :\" + str(temp.numpy())\n            pbar.set_postfix_str(print_statement)\n        \n        loss /= no_of_batches\n        losses.append(loss.numpy())\n        # with file_writer.as_default():\n        #     tf.summary.scalar('Loss', data=loss.numpy(), step=i)\n            \n        print_statement = \"Loss :\" + str(loss.numpy())\n        \n        pbar.set_postfix_str(print_statement)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-23T09:05:10.623607Z","iopub.execute_input":"2022-03-23T09:05:10.624185Z","iopub.status.idle":"2022-03-23T09:06:34.342725Z","shell.execute_reply.started":"2022-03-23T09:05:10.624145Z","shell.execute_reply":"2022-03-23T09:06:34.342016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_generator = DataGenerator(dataset_path='../input/real-time-dataset/dataset4/train/')\ntrain_dict = data_generator.curate_dataset('../input/real-time-dataset/dataset4/train/')\nlabels_train = []\nfeatures_train = []\nimages_train = []\n\nfor k, v in train_dict.items():\n    images = []\n    for e in v:\n        image_path = '../input/real-time-dataset/dataset4/train/' + str(k) + '/' + str(e)\n        image = cv2.imread(image_path)\n        image = np.asarray(image, dtype=np.float64)\n        image = preprocess_input(image)\n        images_train.append(image)\n#         img_features = model.get_features(np.expand_dims(image, axis=0))\n#         features_train.append(img_features[0].numpy())\n        labels_train.append(k)\n    \nimages_train = np.asarray(images_train)\n#features_train = np.asarray(features_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T11:08:55.951593Z","iopub.execute_input":"2022-03-25T11:08:55.951911Z","iopub.status.idle":"2022-03-25T11:09:14.870336Z","shell.execute_reply.started":"2022-03-25T11:08:55.95182Z","shell.execute_reply":"2022-03-25T11:09:14.869598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_generator = DataGenerator(dataset_path='../input/real-time-dataset/dataset4/test/')\ntest_dict = data_generator.curate_dataset('../input/real-time-dataset/dataset4/test/')\nlabels_test = []\nfeatures_test = []\nimages_test = []\n\n\nfor k, v in test_dict.items():\n    if k in train_dict.keys():\n        images = []\n        for e in v:\n            image_path = '../input/real-time-dataset/dataset4/test/' + str(k) + '/' + str(e)\n            image = cv2.imread(image_path)\n            image = np.asarray(image, dtype=np.float64)\n\n            image = preprocess_input(image)\n            images_test.append(image)\n#             img_features = model.get_features(np.expand_dims(image, axis=0))\n#             features_test.append(img_features[0].numpy())\n            labels_test.append(k)\n\n\nimages_test = np.asarray(images_test)\n#features_test = np.asarray(features_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T11:09:14.87194Z","iopub.execute_input":"2022-03-25T11:09:14.872225Z","iopub.status.idle":"2022-03-25T11:09:18.64379Z","shell.execute_reply.started":"2022-03-25T11:09:14.872189Z","shell.execute_reply":"2022-03-25T11:09:18.643069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nle.fit(labels_train)\nlabels_train = le.transform(labels_train)\nlabels_test = le.transform(labels_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T11:09:18.645159Z","iopub.execute_input":"2022-03-25T11:09:18.645404Z","iopub.status.idle":"2022-03-25T11:09:18.657936Z","shell.execute_reply.started":"2022-03-25T11:09:18.645369Z","shell.execute_reply":"2022-03-25T11:09:18.657116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.utils import shuffle\nfeatures_train, labels_train = shuffle(features_train, labels_train)\nfeatures_test, labels_test = shuffle(features_test, labels_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T11:09:18.659454Z","iopub.execute_input":"2022-03-25T11:09:18.659762Z","iopub.status.idle":"2022-03-25T11:09:18.688792Z","shell.execute_reply.started":"2022-03-25T11:09:18.659727Z","shell.execute_reply":"2022-03-25T11:09:18.687903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_classes = len(set(labels_train))","metadata":{"execution":{"iopub.status.busy":"2022-03-23T09:08:16.843917Z","iopub.execute_input":"2022-03-23T09:08:16.844334Z","iopub.status.idle":"2022-03-23T09:08:16.851194Z","shell.execute_reply.started":"2022-03-23T09:08:16.8443Z","shell.execute_reply":"2022-03-23T09:08:16.850013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nclf = SVC(C=10, gamma=1, kernel='rbf',  probability=True)\nclf.fit(features_train, labels_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T09:09:12.868193Z","iopub.execute_input":"2022-03-23T09:09:12.86855Z","iopub.status.idle":"2022-03-23T09:09:17.080249Z","shell.execute_reply.started":"2022-03-23T09:09:12.86851Z","shell.execute_reply":"2022-03-23T09:09:17.079586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\npreds = clf.predict(features_test)\naccuracy_score(labels_test, preds)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T09:10:04.598095Z","iopub.execute_input":"2022-03-23T09:10:04.598648Z","iopub.status.idle":"2022-03-23T09:10:05.225342Z","shell.execute_reply.started":"2022-03-23T09:10:04.598611Z","shell.execute_reply":"2022-03-23T09:10:05.224625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-03-23T09:11:05.265534Z","iopub.execute_input":"2022-03-23T09:11:05.265802Z","iopub.status.idle":"2022-03-23T09:11:05.275484Z","shell.execute_reply.started":"2022-03-23T09:11:05.265772Z","shell.execute_reply":"2022-03-23T09:11:05.274559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_from_scratch():\t\n    model = Sequential()\n    model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n    model.add(Convolution2D(64, (3, 3), activation='relu'))\n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2,2), strides=(2,2)))\n    \n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(128, (3, 3), activation='relu'))\n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(128, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2,2), strides=(2,2)))\n    \n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(256, (3, 3), activation='relu'))\n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(256, (3, 3), activation='relu'))\n#     model.add(ZeroPadding2D((1,1)))\n#     model.add(Convolution2D(256, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2,2), strides=(2,2)))\n    \n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(512, (3, 3), activation='relu'))\n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(512, (3, 3), activation='relu'))\n#     model.add(ZeroPadding2D((1,1)))\n#     model.add(Convolution2D(512, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2,2), strides=(2,2)))\n    \n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(512, (3, 3), activation='relu'))\n#     model.add(ZeroPadding2D((1,1)))\n#     model.add(Convolution2D(512, (3, 3), activation='relu'))\n#     model.add(ZeroPadding2D((1,1)))\n#     model.add(Convolution2D(512, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2,2), strides=(2,2)))\n    \n#     model.add(Convolution2D(4096, (7, 7), activation='relu'))\n#     model.add(Dropout(0.5))\n#     model.add(Convolution2D(4096, (1, 1), activation='relu'))\n#     model.add(Dropout(0.5))\n#     model.add(Convolution2D(2622, (1, 1)))\n    model.add(Flatten())\n    model.add(layers.Dense(2*1024, activation='relu'))\n    model.add(layers.Dropout(0.5)) \n    model.add(layers.Dense(1024, activation='relu'))\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(1024//2, activation='relu'))\n    model.add(layers.Dropout(0.5)) \n    \n    # Number of classes !!!\n    model.add(Dense(n_classes))\n    \n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"student = model_from_scratch()\nstudent.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class Distiller(keras.Model):\n#     def __init__(self, student, teacher_part1, teacher_part2):\n#         super(Distiller, self).__init__()\n#         self.teacher_part1 = teacher_part1\n#         self.teacher_part2 = teacher_part2\n#         self.student = student\n\n#     def compile(\n#         self,\n#         optimizer,\n#         metrics,\n#         student_loss_fn,\n#         distillation_loss_fn,\n#         alpha=0.1,\n#         temperature=3,\n#     ):\n#         \"\"\" Configure the distiller.\n\n#         Args:\n#             optimizer: Keras optimizer for the student weights\n#             metrics: Keras metrics for evaluation\n#             student_loss_fn: Loss function of difference between student\n#                 predictions and ground-truth\n#             distillation_loss_fn: Loss function of difference between soft\n#                 student predictions and soft teacher predictions\n#             alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n#             temperature: Temperature for softening probability distributions.\n#                 Larger temperature gives softer distributions.\n#         \"\"\"\n#         super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\n#         self.student_loss_fn = student_loss_fn\n#         self.distillation_loss_fn = distillation_loss_fn\n#         self.alpha = alpha\n#         self.temperature = temperature\n    \n#     def call(self, x):\n#         return self.student(x)\n\n#     def train_step(self, data):\n#         # Unpack data\n#         x, y = data\n#         print(x)\n#         # Forward pass of teacher\n#         features = self.teacher_part1.get_features(x)\n#         teacher_predictions = self.teacher_part2(features, training=False)\n#         print(features.shape)\n#         print(teacher_predictions.shape)\n#         #teacher_predictions = tf.convert_to_tensor(teacher_predictions)\n        \n#         with tf.GradientTape() as tape:\n#             # Forward pass of student\n#             student_predictions = self.student(x, training=True)\n#             # Compute losses\n#             student_loss = self.student_loss_fn(y, student_predictions)\n#             distillation_loss = self.distillation_loss_fn(\n#                 tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n#                 tf.nn.softmax(student_predictions / self.temperature, axis=1),\n#             )\n#             loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n\n#         # Compute gradients\n#         trainable_vars = self.student.trainable_variables\n#         gradients = tape.gradient(loss, trainable_vars)\n\n#         # Update weights\n#         self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n\n#         # Update the metrics configured in `compile()`.\n#         self.compiled_metrics.update_state(y, student_predictions)\n\n#         # Return a dict of performance\n#         results = {m.name: m.result() for m in self.metrics}\n#         results.update(\n#             {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n#         )\n#         return results\n\n#     def test_step(self, data):\n#         # Unpack the data\n#         x, y = data\n\n#         # Compute predictions\n#         y_prediction = self.student(x, training=False)\n\n#         # Calculate the loss\n#         student_loss = self.student_loss_fn(y, y_prediction)\n\n#         # Update the metrics.\n#         self.compiled_metrics.update_state(y, y_prediction)\n\n#         # Return a dict of performance\n#         results = {m.name: m.result() for m in self.metrics}\n#         results.update({\"student_loss\": student_loss})\n#         return results","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# distiller = Distiller(student=student, teacher_part1=model, teacher_part2=clf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def classifier():\n    clf = keras.Sequential()\n    \n    clf.add(layers.Dense(1024//2, activation='relu', input_dim=128))\n    clf.add(layers.Dropout(0.5))\n    clf.add(layers.Dense(1024//4, activation='relu'))\n    clf.add(layers.Dropout(0.5)) \n    # Number of classes !!!\n    clf.add(Dense(n_classes))\n    return clf\nclf = classifier()\nclf.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf.compile(optimizer=tf.keras.optimizers.Adam(lr=0.00006), loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics = [tf.keras.metrics.SparseCategoricalAccuracy()])\nclf_hist = clf.fit(features_train, labels_train, validation_split=0.2,epochs = 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nclf.evaluate(features_test, labels_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DataGenerator for fit","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport keras\n\nclass DataGenerator(tf.keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, list_IDs, labels, batch_size=15, dim=(224,224), n_channels=3,\n                 n_classes=10, shuffle=True):\n        'Initialization'\n        self.dim = dim\n        self.batch_size = batch_size\n        self.labels = labels\n        self.list_IDs = list_IDs\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.shuffle = shuffle\n        self.on_epoch_end()\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.list_IDs) / self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        # Find list of IDs\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n\n        # Generate data\n        X, y = self.__data_generation(list_IDs_temp)\n\n        return X, y\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def __data_generation(self, list_IDs_temp):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n        y = np.empty((self.batch_size), dtype=int)\n\n        # Generate data\n        for i, ID in enumerate(list_IDs_temp):\n            # Store sample\n            img = cv2.imread('../input/big-dataset/img_align_celeba/' + str(ID))\n            img = cv2.resize(img, (224, 224))\n            img = np.asarray(img, dtype=np.float64)\n            img = preprocess_input(img)\n            X[i,] = img\n\n            # Store class\n            y[i] = self.labels[ID]\n\n        return X, y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('../input/labels-boxes/identity_CelebA.txt') as f:\n    lines_id = f.readlines()\n\nlabels = {}\nIds = []\nfor e in lines_id:\n    labels[e.split()[0]] = int(e.split()[1])\n    Ids.append(e.split()[0])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('../input/labels-boxes/identity_CelebA.txt') as f:\n    lines_id = f.readlines()\n\nlabels = []\nimages = []\nfor e in lines_id:\n    labels.append(int(e.split()[1]))\n    img = cv2.imread('../input/big-dataset/img_align_celeba/' + e.split()[0])\n    img = cv2.resize(img, (224, 224))\n    img = np.asarray(img, dtype=np.float64)\n    img = preprocess_input(img)\n    images.append(img)\n# with open('../input/labels-boxes/list_bbox_celeba.txt') as f:\n#     lines_b = f.readlines()\n# boxes = [[int(e.split()[i]) for i in range(1,5)] for e in lines_b[2:]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.utils import shuffle\nimages, labels = shuffle(images, labels)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Teacher(keras.Model):\n    def __init__(self, teacher_part1, teacher_part2):\n        super(Teacher, self).__init__()\n        self.teacher_part1 = teacher_part1\n        self.teacher_part2 = teacher_part2\n    \n    def call(self, inputs):\n        x = self.teacher_part1(inputs, training=False)\n        return self.teacher_part2(x, training=False)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = vgg_face()\nmodel.load_weights('../input/weights/vgg_face_weights.h5')\nmodel.pop()\nmodel.add(tf.keras.layers.Dense(128, use_bias=False))\nfor layer in model.layers[:-2]:\n    layer.trainable = False","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"teacher = Teacher(teacher_part1 = model, teacher_part2=clf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass Distiller(keras.Model):\n    def __init__(self, student, teacher):\n        super(Distiller, self).__init__()\n        self.teacher = teacher\n        self.student = student\n\n    def compile(\n        self,\n        optimizer,\n        metrics,\n        student_loss_fn,\n        distillation_loss_fn,\n        alpha=0.1,\n        temperature=3,\n    ):\n        \"\"\" Configure the distiller.\n\n        Args:\n            optimizer: Keras optimizer for the student weights\n            metrics: Keras metrics for evaluation\n            student_loss_fn: Loss function of difference between student\n                predictions and ground-truth\n            distillation_loss_fn: Loss function of difference between soft\n                student predictions and soft teacher predictions\n            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n            temperature: Temperature for softening probability distributions.\n                Larger temperature gives softer distributions.\n        \"\"\"\n        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\n        self.student_loss_fn = student_loss_fn\n        self.distillation_loss_fn = distillation_loss_fn\n        self.alpha = alpha\n        self.temperature = temperature\n\n#     def call(self, inputs):\n#         return self.student(inputs)\n    \n    def train_step(self, data):\n        # Unpack data\n        x, y = data\n        print(data)\n        # Forward pass of teacher\n        teacher_predictions = self.teacher(x)\n\n        with tf.GradientTape() as tape:\n            # Forward pass of student\n            student_predictions = self.student(x, training=True)\n\n            # Compute losses\n            student_loss = self.student_loss_fn(y, student_predictions)\n            distillation_loss = self.distillation_loss_fn(\n                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n                tf.nn.softmax(student_predictions / self.temperature, axis=1),\n            )\n            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n            \n        # Compute gradients\n        trainable_vars = self.student.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n\n        # Update weights\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n\n        # Update the metrics configured in `compile()`.\n        self.compiled_metrics.update_state(y, student_predictions)\n\n        # Return a dict of performance\n        results = {m.name: m.result() for m in self.metrics}\n        results.update(\n            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n        )\n        return results\n\n    def test_step(self, data):\n        # Unpack the data\n        x, y = data\n\n        # Compute predictions\n        y_prediction = self.student(x, training=False)\n\n        # Calculate the loss\n        student_loss = self.student_loss_fn(y, y_prediction)\n\n        # Update the metrics.\n        self.compiled_metrics.update_state(y, y_prediction)\n\n        # Return a dict of performance\n        results = {m.name: m.result() for m in self.metrics}\n        results.update({\"student_loss\": student_loss})\n        return results\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"distiller = Distiller(student=student, teacher=teacher)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#datagen = DataGenerator(list_IDs=Ids, labels=labels, n_classes=len(set(labels.values())))\n\ndistiller.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.00006),\n    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    distillation_loss_fn=keras.losses.KLDivergence(),\n    alpha=0.5,\n    temperature=10,\n)\n# Distill teacher to student\ndistiller.fit(images_train, labels_train, epochs=100, validation_split=0.2)\n\n# Evaluate student on test dataset\ndistiller.evaluate(images_test, labels_test)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}