{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-25T09:02:22.363412Z","iopub.execute_input":"2022-03-25T09:02:22.36398Z","iopub.status.idle":"2022-03-25T09:02:22.389866Z","shell.execute_reply.started":"2022-03-25T09:02:22.363891Z","shell.execute_reply":"2022-03-25T09:02:22.38916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# import librairies","metadata":{}},{"cell_type":"code","source":"# pip install tensorflow==2.3","metadata":{"execution":{"iopub.status.busy":"2022-03-25T09:02:24.385568Z","iopub.execute_input":"2022-03-25T09:02:24.385821Z","iopub.status.idle":"2022-03-25T09:02:24.390201Z","shell.execute_reply.started":"2022-03-25T09:02:24.385793Z","shell.execute_reply":"2022-03-25T09:02:24.389011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\ntf.__version__","metadata":{"execution":{"iopub.status.busy":"2022-03-25T09:02:25.702953Z","iopub.execute_input":"2022-03-25T09:02:25.703477Z","iopub.status.idle":"2022-03-25T09:02:30.159683Z","shell.execute_reply.started":"2022-03-25T09:02:25.703442Z","shell.execute_reply":"2022-03-25T09:02:30.158998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport io\nimport cv2\nimport numpy as np\nfrom os import listdir\nfrom os.path import isfile, join\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn import svm\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\n\n#import keras\nimport tensorflow as tf\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.applications import ResNet50\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\nfrom skimage.feature import hog\nfrom skimage import data, exposure\n\nimport tensorflow_addons as tfa\nimport random\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2022-03-25T09:02:39.350093Z","iopub.execute_input":"2022-03-25T09:02:39.35058Z","iopub.status.idle":"2022-03-25T09:02:39.744269Z","shell.execute_reply.started":"2022-03-25T09:02:39.350546Z","shell.execute_reply":"2022-03-25T09:02:39.743303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install kerassurgeon","metadata":{"execution":{"iopub.status.busy":"2022-03-25T09:02:32.566322Z","iopub.status.idle":"2022-03-25T09:02:32.566932Z","shell.execute_reply.started":"2022-03-25T09:02:32.566689Z","shell.execute_reply":"2022-03-25T09:02:32.566715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install tensorflow_addons\n# !pip install keras_applications \n# !pip install keras_preprocessing \n# !pip install git+https://github.com/rcmalli/keras-vggface.git","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Classes and functions","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import backend as K\ndef preprocess_input(x, data_format=None, version=1):\n    x_temp = np.copy(x)\n    if data_format is None:\n        data_format = K.image_data_format()\n    assert data_format in {'channels_last', 'channels_first'}\n\n    if version == 1:\n        if data_format == 'channels_first':\n            x_temp = x_temp[:, ::-1, ...]\n            x_temp[:, 0, :, :] -= 93.5940\n            x_temp[:, 1, :, :] -= 104.7624\n            x_temp[:, 2, :, :] -= 129.1863\n        else:\n            x_temp = x_temp[..., ::-1]\n            x_temp[..., 0] -= 93.5940\n            x_temp[..., 1] -= 104.7624\n            x_temp[..., 2] -= 129.1863\n\n    elif version == 2:\n        if data_format == 'channels_first':\n            x_temp = x_temp[:, ::-1, ...]\n            x_temp[:, 0, :, :] -= 91.4953\n            x_temp[:, 1, :, :] -= 103.8827\n            x_temp[:, 2, :, :] -= 131.0912\n        else:\n            x_temp = x_temp[..., ::-1]\n            x_temp[..., 0] -= 91.4953\n            x_temp[..., 1] -= 103.8827\n            x_temp[..., 2] -= 131.0912\n    else:\n        raise NotImplementedError\n\n    return x_temp","metadata":{"execution":{"iopub.status.busy":"2022-03-25T09:02:45.462706Z","iopub.execute_input":"2022-03-25T09:02:45.463036Z","iopub.status.idle":"2022-03-25T09:02:45.47378Z","shell.execute_reply.started":"2022-03-25T09:02:45.463Z","shell.execute_reply":"2022-03-25T09:02:45.472635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataGenerator(tf.keras.utils.Sequence):\n    def __init__(self, dataset_path, batch_size=5, shuffle=True):\n        self.dataset = self.curate_dataset(dataset_path)\n        self.dataset_path = dataset_path\n        self.shuffle = shuffle\n        self.batch_size =batch_size\n        self.no_of_people = len(list(self.dataset.keys()))\n        self.on_epoch_end()\n        print(self.dataset.keys())\n        \n    def __getitem__(self, index):\n        people = list(self.dataset.keys())[index * self.batch_size: (index + 1) * self.batch_size]\n        P = []\n        A = []\n        N = []\n        \n        for person in people:\n            anchor_index = random.randint(0, len(self.dataset[person])-1)\n            a = self.get_image(person, anchor_index)\n            \n            positive_index = random.randint(0, len(self.dataset[person])-1)\n            while positive_index == anchor_index and len(self.dataset[person]) != 1:\n                positive_index = random.randint(0, len(self.dataset[person])-1)\n            p = self.get_image(person, positive_index)\n            \n            negative_person_index = random.randint(0, self.no_of_people - 1)\n            negative_person = list(self.dataset.keys())[negative_person_index]\n            while negative_person == person:\n                negative_person_index = random.randint(0, self.no_of_people - 1)\n                negative_person = list(self.dataset.keys())[negative_person_index]\n            \n            negative_index = random.randint(0, len(self.dataset[negative_person])-1)\n            n = self.get_image(negative_person, negative_index)\n            P.append(p)\n            A.append(a)\n            N.append(n)\n        A = np.asarray(A)\n        N = np.asarray(N)\n        P = np.asarray(P)\n        return [A, P, N]\n        \n    def __len__(self):\n        return self.no_of_people // self.batch_size\n        \n    def curate_dataset(self, dataset_path):\n        dataset = {}\n        dirs = [dir for dir in listdir(dataset_path)]\n        for dir in dirs: \n            fichiers = [f for f in listdir(dataset_path+dir) if \"jpeg\" in f or \"png\" in f]\n            for f in fichiers:\n                if dir in dataset.keys():\n                    dataset[dir].append(f)\n                else:\n                    dataset[dir] = [f]\n        return dataset\n    \n    def on_epoch_end(self):\n        if self.shuffle:\n            keys = list(self.dataset.keys())\n            random.shuffle(keys)\n            dataset_ =  {}\n            for key in keys:\n                dataset_[key] = self.dataset[key]\n            self.dataset = dataset_\n            \n    def get_image(self, person, index):\n        img = cv2.imread(os.path.join(self.dataset_path, os.path.join(person, self.dataset[person][index])))\n        img = cv2.resize(img, (224, 224))\n        img = np.asarray(img, dtype=np.float64)\n        img = preprocess_input(img)\n        return img","metadata":{"execution":{"iopub.status.busy":"2022-03-25T09:02:48.038981Z","iopub.execute_input":"2022-03-25T09:02:48.03941Z","iopub.status.idle":"2022-03-25T09:02:48.062492Z","shell.execute_reply.started":"2022-03-25T09:02:48.039373Z","shell.execute_reply":"2022-03-25T09:02:48.061544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SiameseNetwork(tf.keras.Model):\n    def __init__(self, vgg_face):\n        super(SiameseNetwork, self).__init__()\n        self.vgg_face = vgg_face\n        \n    @tf.function\n    def call(self, inputs):\n        image_1, image_2, image_3 =  inputs\n        with tf.name_scope(\"Anchor\") as scope:\n            feature_1 = self.vgg_face(image_1)\n            feature_1 = tf.math.l2_normalize(feature_1, axis=-1)\n        with tf.name_scope(\"Positive\") as scope:\n            feature_2 = self.vgg_face(image_2)\n            feature_2 = tf.math.l2_normalize(feature_2, axis=-1)\n        with tf.name_scope(\"Negative\") as scope:\n            feature_3 = self.vgg_face(image_3)\n            feature_3 = tf.math.l2_normalize(feature_3, axis=-1)\n        return [feature_1, feature_2, feature_3]\n    \n    @tf.function\n    def get_features(self, inputs):\n        return tf.math.l2_normalize(self.vgg_face(inputs, training=False), axis=-1)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T09:02:48.987788Z","iopub.execute_input":"2022-03-25T09:02:48.988376Z","iopub.status.idle":"2022-03-25T09:02:48.999993Z","shell.execute_reply.started":"2022-03-25T09:02:48.98834Z","shell.execute_reply":"2022-03-25T09:02:48.999155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"K = tf.keras.backend\ndef loss_function(x, alpha = 0.2):\n    # Triplet Loss function.\n    anchor,positive,negative = x\n    # distance between the anchor and the positive\n    pos_dist = K.sum(K.square(anchor-positive),axis=1)\n    # distance between the anchor and the negative\n    neg_dist = K.sum(K.square(anchor-negative),axis=1)\n    # compute loss\n    basic_loss = pos_dist-neg_dist+alpha\n    loss = K.mean(K.maximum(basic_loss,0.0))\n    return loss","metadata":{"execution":{"iopub.status.busy":"2022-03-25T09:02:50.201783Z","iopub.execute_input":"2022-03-25T09:02:50.202316Z","iopub.status.idle":"2022-03-25T09:02:50.207769Z","shell.execute_reply.started":"2022-03-25T09:02:50.202284Z","shell.execute_reply":"2022-03-25T09:02:50.206771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(learning_rate=0.00006)\n#binary_cross_entropy = tf.keras.losses.BinaryCrossentropy()\ndef train(X):\n    with tf.GradientTape() as tape:\n        y_pred = model(X)\n        loss = loss_function(y_pred)\n    grad = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(grad, model.trainable_variables))\n    return loss","metadata":{"execution":{"iopub.status.busy":"2022-03-25T09:02:51.333621Z","iopub.execute_input":"2022-03-25T09:02:51.334139Z","iopub.status.idle":"2022-03-25T09:02:51.344878Z","shell.execute_reply.started":"2022-03-25T09:02:51.3341Z","shell.execute_reply":"2022-03-25T09:02:51.344243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pruning","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import ZeroPadding2D, Convolution2D, MaxPooling2D, Dropout, Flatten, Activation\n\ndef vgg_face():\t\n    model = Sequential()\n    model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n    model.add(Convolution2D(64, (3, 3), activation='relu'))\n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2,2), strides=(2,2)))\n    \n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(128, (3, 3), activation='relu'))\n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(128, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2,2), strides=(2,2)))\n    \n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(256, (3, 3), activation='relu'))\n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(256, (3, 3), activation='relu'))\n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(256, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2,2), strides=(2,2)))\n    \n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(512, (3, 3), activation='relu'))\n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(512, (3, 3), activation='relu'))\n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(512, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2,2), strides=(2,2)))\n    \n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(512, (3, 3), activation='relu'))\n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(512, (3, 3), activation='relu'))\n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(512, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2,2), strides=(2,2)))\n    \n    model.add(Convolution2D(4096, (7, 7), activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Convolution2D(4096, (1, 1), activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Convolution2D(2622, (1, 1)))\n    model.add(Flatten())\n    model.add(Activation('softmax'))\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-03-25T09:02:53.971193Z","iopub.execute_input":"2022-03-25T09:02:53.971731Z","iopub.status.idle":"2022-03-25T09:02:53.993551Z","shell.execute_reply.started":"2022-03-25T09:02:53.971696Z","shell.execute_reply":"2022-03-25T09:02:53.992717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vgg = vgg_face()\nvgg.load_weights('../input/weights/vgg_face_weights.h5')\nvgg.pop()","metadata":{"execution":{"iopub.status.busy":"2022-03-23T15:47:13.633952Z","iopub.execute_input":"2022-03-23T15:47:13.636411Z","iopub.status.idle":"2022-03-23T15:47:13.941897Z","shell.execute_reply.started":"2022-03-23T15:47:13.636374Z","shell.execute_reply":"2022-03-23T15:47:13.940271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model\nfrom kerassurgeon import Surgeon, identify\nfrom kerassurgeon.operations import delete_channels, delete_layer\nimport os\nimport numpy as np\nimport math\n  \ndef get_filter_weights(model, layer=None):\n    \"\"\"function to return weights array for one or all conv layers of a Keras model\"\"\"\n    if layer or layer==0:\n        weight_array = model.layers[layer].get_weights()[0]\n        \n    else:\n        weights = [model.layers[layer_ix].get_weights()[0] for layer_ix in range(len(model.layers))\\\n         if 'conv' in model.layers[layer_ix].name]\n        weight_array = [np.array(i) for i in weights]\n    \n    return weight_array\n\ndef get_filters_l1(model, layer=None):\n    \"\"\"Returns L1 norm of a Keras model filters at a given conv layer, if layer=None, returns a matrix of norms\nmodel is a Keras model\"\"\"\n    if layer or layer==0:\n        weights = get_filter_weights(model, layer)\n        num_filter = len(weights[0,0,0,:])\n        norms_dict = {}\n        norms = []\n        for i in range(num_filter):\n            l1_norm = np.sum(abs(weights[:,:,:,i]))\n            norms.append(l1_norm)\n    else:\n        weights = get_filter_weights(model)\n        max_kernels = max([layr.shape[3] for layr in weights])\n        norms = np.empty((len(weights), max_kernels))\n        norms[:] = np.NaN\n        for layer_ix in range(len(weights)):\n            # compute norm of the filters\n            kernel_size = weights[layer_ix][:,:,:,0].size\n            nb_filters = weights[layer_ix].shape[3]\n            kernels = weights[layer_ix]\n            l1 = [np.sum(abs(kernels[:,:,:,i])) for i in range(nb_filters)]\n            # divide by shape of the filters\n            l1 = np.array(l1) / kernel_size\n            norms[layer_ix, :nb_filters] = l1\n    return norms","metadata":{"execution":{"iopub.status.busy":"2022-03-23T15:41:40.754314Z","iopub.execute_input":"2022-03-23T15:41:40.755058Z","iopub.status.idle":"2022-03-23T15:41:40.791495Z","shell.execute_reply.started":"2022-03-23T15:41:40.755005Z","shell.execute_reply":"2022-03-23T15:41:40.790777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_pruned_count(model, perc=0.1, layer=None):\n    if layer or layer ==0:\n        # count nb of filters\n        nb_filters = model.layers[layer].output_shape[3]\n    else:\n        nb_filters = np.sum([model.layers[i].output_shape[3] for i, layer in enumerate(model.layers) \n                                if 'conv' in model.layers[i].name])\n            \n    n_pruned = int(np.floor(perc*nb_filters))\n    return n_pruned\n\n\ndef smallest_indices(array, N):\n    idx = array.ravel().argsort()[:N]\n    return np.stack(np.unravel_index(idx, array.shape)).T\n\ndef biggest_indices(array, N):\n    idx = array.ravel().argsort()[::-1][:N]\n    return np.stack(np.unravel_index(idx, array.shape)).T","metadata":{"execution":{"iopub.status.busy":"2022-03-23T15:41:41.649022Z","iopub.execute_input":"2022-03-23T15:41:41.649787Z","iopub.status.idle":"2022-03-23T15:41:41.659863Z","shell.execute_reply.started":"2022-03-23T15:41:41.649748Z","shell.execute_reply":"2022-03-23T15:41:41.65899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kerassurgeon.operations import delete_channels, delete_layer\nfrom kerassurgeon import Surgeon\n\ndef prune_one_layer(model, pruned_indexes, layer_ix, opt):\n    \"\"\"Prunes one layer based on a Keras Model, layer index \n    and indexes of filters to prune\"\"\"\n    model_pruned = delete_channels(model, model.layers[layer_ix], pruned_indexes)\n    model_pruned.compile(loss='categorical_crossentropy',\n                          optimizer=opt,\n                          metrics=['accuracy'])\n    return model_pruned\n\ndef prune_multiple_layers(model, pruned_matrix, opt):\n    \"\"\"Prunes several layers based on a Keras Model, layer index and matrix \n      of indexes of filters to prune\"\"\"\n    conv_indexes = [i for i, v in enumerate(model.layers) if 'conv' in v.name]\n    layers_to_prune = np.unique(pruned_matrix[:,0])\n    surgeon = Surgeon(model, copy=True)\n    to_prune = pruned_matrix\n    to_prune[:,0] = np.array([conv_indexes[i] for i in to_prune[:,0]])\n    layers_to_prune = np.unique(to_prune[:,0])\n    for layer_ix in layers_to_prune :\n        pruned_filters = [x[1] for x in to_prune if x[0]==layer_ix]\n        pruned_layer = model.layers[layer_ix]\n        if pruned_layer.name == 'conv2d_14':\n            surgeon.add_job('delete_layer', pruned_layer)\n            surgeon.add_job('delete_layer', model.layers[layer_ix+1])\n        \n        elif pruned_layer.name == 'conv2d_13':\n            pass\n        else: \n            surgeon.add_job('delete_channels', pruned_layer, channels=pruned_filters)\n    \n    model_pruned = surgeon.operate()\n    model_pruned.compile(loss='categorical_crossentropy',\n              optimizer=opt,\n              metrics=['accuracy'])\n    \n    return model_pruned","metadata":{"execution":{"iopub.status.busy":"2022-03-23T15:41:42.575883Z","iopub.execute_input":"2022-03-23T15:41:42.576165Z","iopub.status.idle":"2022-03-23T15:41:42.586331Z","shell.execute_reply.started":"2022-03-23T15:41:42.576133Z","shell.execute_reply":"2022-03-23T15:41:42.585506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prune_model(model, perc, opt, layer=None):\n    \"\"\"Prune a Keras model using different methods\n    Arguments:\n        model: Keras Model object\n        perc: a float between 0 and 1\n        method: method to prune, can be one of ['l1','apoz','random']\n    Returns:\n        A pruned Keras Model object\n    \n    \"\"\"\n    assert perc >=0 and perc <1, \"Invalid pruning percentage\"\n    \n    \n    n_pruned = compute_pruned_count(model, perc, layer)\n    filters = get_filters_l1(model)\n    \n    to_prune = smallest_indices(filters, n_pruned)\n    if layer or layer ==0:\n        model_pruned = prune_one_layer(model, to_prune, layer, opt)\n    else:\n        model_pruned = prune_multiple_layers(model, to_prune, opt)\n            \n    return model_pruned","metadata":{"execution":{"iopub.status.busy":"2022-03-23T15:41:44.188373Z","iopub.execute_input":"2022-03-23T15:41:44.188674Z","iopub.status.idle":"2022-03-23T15:41:44.196513Z","shell.execute_reply.started":"2022-03-23T15:41:44.188639Z","shell.execute_reply":"2022-03-23T15:41:44.195132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pruned_model = prune_model(vgg, 0.65, tf.keras.optimizers.Adam(learning_rate=0.00006))","metadata":{"execution":{"iopub.status.busy":"2022-03-23T15:42:21.164751Z","iopub.execute_input":"2022-03-23T15:42:21.165028Z","iopub.status.idle":"2022-03-23T15:42:29.456933Z","shell.execute_reply.started":"2022-03-23T15:42:21.164996Z","shell.execute_reply":"2022-03-23T15:42:29.455877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pruned_model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-03-23T15:42:29.462183Z","iopub.execute_input":"2022-03-23T15:42:29.463009Z","iopub.status.idle":"2022-03-23T15:42:29.492839Z","shell.execute_reply.started":"2022-03-23T15:42:29.462922Z","shell.execute_reply":"2022-03-23T15:42:29.492016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pruned_model.save(\"0.65sparcity_base_model.h5\")","metadata":{"execution":{"iopub.status.busy":"2022-03-23T15:42:50.549628Z","iopub.execute_input":"2022-03-23T15:42:50.550143Z","iopub.status.idle":"2022-03-23T15:42:50.929714Z","shell.execute_reply.started":"2022-03-23T15:42:50.550104Z","shell.execute_reply":"2022-03-23T15:42:50.928948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test our pruned model","metadata":{}},{"cell_type":"code","source":"# model = vgg_face()\n# model.load_weights('../input/weights/vgg_face_weights.h5')\n# model.pop()\n# model.add(tf.keras.layers.Dense(128, use_bias=False))\n# for layer in model.layers[:-2]:\n#     layer.trainable = False\n# model = SiameseNetwork(model)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T09:36:50.335624Z","iopub.execute_input":"2022-03-25T09:36:50.335872Z","iopub.status.idle":"2022-03-25T09:36:56.416708Z","shell.execute_reply.started":"2022-03-25T09:36:50.335844Z","shell.execute_reply":"2022-03-25T09:36:56.415912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import keras\npruned_model = keras.models.load_model(\"../input/065weights/0.65sparcity_base_model.h5\")","metadata":{"execution":{"iopub.status.busy":"2022-03-25T09:47:46.20961Z","iopub.execute_input":"2022-03-25T09:47:46.209866Z","iopub.status.idle":"2022-03-25T09:47:49.897134Z","shell.execute_reply.started":"2022-03-25T09:47:46.209838Z","shell.execute_reply":"2022-03-25T09:47:49.896425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for layer in pruned_model.layers:\n    layer.trainable = False\n","metadata":{"execution":{"iopub.status.busy":"2022-03-25T09:47:49.898847Z","iopub.execute_input":"2022-03-25T09:47:49.899084Z","iopub.status.idle":"2022-03-25T09:47:49.904087Z","shell.execute_reply.started":"2022-03-25T09:47:49.89905Z","shell.execute_reply":"2022-03-25T09:47:49.90343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import models\nmodel_tl = models.Sequential()\nmodel_tl.add(pruned_model)\nmodel_tl.add(tf.keras.layers.Dense(128, use_bias=False))","metadata":{"execution":{"iopub.status.busy":"2022-03-25T09:47:51.201192Z","iopub.execute_input":"2022-03-25T09:47:51.201844Z","iopub.status.idle":"2022-03-25T09:47:51.281745Z","shell.execute_reply.started":"2022-03-25T09:47:51.201808Z","shell.execute_reply":"2022-03-25T09:47:51.281053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_tl.summary()","metadata":{"execution":{"iopub.status.busy":"2022-03-25T09:47:54.866727Z","iopub.execute_input":"2022-03-25T09:47:54.867375Z","iopub.status.idle":"2022-03-25T09:47:54.875181Z","shell.execute_reply.started":"2022-03-25T09:47:54.867336Z","shell.execute_reply":"2022-03-25T09:47:54.874364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = SiameseNetwork(model_tl)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T09:47:56.856905Z","iopub.execute_input":"2022-03-25T09:47:56.857198Z","iopub.status.idle":"2022-03-25T09:47:56.876174Z","shell.execute_reply.started":"2022-03-25T09:47:56.857163Z","shell.execute_reply":"2022-03-25T09:47:56.875508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_generator = DataGenerator(dataset_path='../input/dataset5/dataset3/train/', batch_size=10)\n\n\nlosses = []\naccuracy = []\nepochs = 50\nno_of_batches = data_generator.__len__()\nfor i in range(1, epochs+1, 1):\n    loss = 0\n    with tqdm(total=no_of_batches) as pbar:\n        \n        description = \"Epoch \" + str(i) + \"/\" + str(epochs)\n        pbar.set_description_str(description)\n        \n        for j in range(no_of_batches):\n            data = data_generator[j]\n            temp = train(data)\n            loss += temp\n            \n            pbar.update()\n            print_statement = \"Loss :\" + str(temp.numpy())\n            pbar.set_postfix_str(print_statement)\n        \n        loss /= no_of_batches\n        losses.append(loss.numpy())\n        # with file_writer.as_default():\n        #     tf.summary.scalar('Loss', data=loss.numpy(), step=i)\n            \n        print_statement = \"Loss :\" + str(loss.numpy())\n        \n        pbar.set_postfix_str(print_statement)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-25T09:03:30.524682Z","iopub.execute_input":"2022-03-25T09:03:30.524938Z","iopub.status.idle":"2022-03-25T09:14:45.276961Z","shell.execute_reply.started":"2022-03-25T09:03:30.524911Z","shell.execute_reply":"2022-03-25T09:14:45.276271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_generator = DataGenerator(dataset_path='../input/dataset6/dataset4/train/')\ntrain_dict = data_generator.curate_dataset('../input/dataset6/dataset4/train/')\nlabels_train = []\nfeatures_train = []\nimages_train = []\n\nfor k, v in train_dict.items():\n    images = []\n    for e in v:\n        image_path = '../input/dataset6/dataset4/train/' + str(k) + '/' + str(e)\n        image = cv2.imread(image_path)\n        image = np.asarray(image, dtype=np.float64)\n        image = preprocess_input(image)\n        images_train.append(image)\n        img_features = model.get_features(np.expand_dims(image, axis=0))\n        features_train.append(img_features[0].numpy())\n        labels_train.append(k)\n    \nimages_train = np.asarray(images_train)\nfeatures_train = np.asarray(features_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T09:48:00.188467Z","iopub.execute_input":"2022-03-25T09:48:00.188942Z","iopub.status.idle":"2022-03-25T09:49:21.348385Z","shell.execute_reply.started":"2022-03-25T09:48:00.188904Z","shell.execute_reply":"2022-03-25T09:49:21.347648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_generator = DataGenerator(dataset_path='../input/dataset6/dataset4/test/')\ntest_dict = data_generator.curate_dataset('../input/dataset6/dataset4/test/')\nlabels_test = []\nfeatures_test = []\nimages_test = []\n\n\nfor k, v in test_dict.items():\n    if k in train_dict.keys():\n        images = []\n        for e in v:\n            image_path = '../input/dataset6/dataset4/test/' + str(k) + '/' + str(e)\n            image = cv2.imread(image_path)\n            image = np.asarray(image, dtype=np.float64)\n\n            image = preprocess_input(image)\n            images_test.append(image)\n            img_features = model.get_features(np.expand_dims(image, axis=0))\n            features_test.append(img_features[0].numpy())\n            labels_test.append(k)\n\n\nimages_test = np.asarray(images_test)\nfeatures_test = np.asarray(features_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T09:49:21.349874Z","iopub.execute_input":"2022-03-25T09:49:21.3501Z","iopub.status.idle":"2022-03-25T09:49:34.574024Z","shell.execute_reply.started":"2022-03-25T09:49:21.350068Z","shell.execute_reply":"2022-03-25T09:49:34.573274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nle.fit(labels_train)\nlabels_train = le.transform(labels_train)\nlabels_test = le.transform(labels_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T09:49:34.575387Z","iopub.execute_input":"2022-03-25T09:49:34.575618Z","iopub.status.idle":"2022-03-25T09:49:34.590475Z","shell.execute_reply.started":"2022-03-25T09:49:34.575586Z","shell.execute_reply":"2022-03-25T09:49:34.58981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.utils import shuffle\nfeatures_train, labels_train = shuffle(features_train, labels_train)\nfeatures_test, labels_test = shuffle(features_test, labels_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T09:49:34.592405Z","iopub.execute_input":"2022-03-25T09:49:34.592726Z","iopub.status.idle":"2022-03-25T09:49:34.598566Z","shell.execute_reply.started":"2022-03-25T09:49:34.592683Z","shell.execute_reply":"2022-03-25T09:49:34.597786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nclf = SVC(C=10, gamma=1, kernel='rbf',  probability=True)\nclf.fit(features_train, labels_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T09:49:34.599749Z","iopub.execute_input":"2022-03-25T09:49:34.600418Z","iopub.status.idle":"2022-03-25T09:49:47.206106Z","shell.execute_reply.started":"2022-03-25T09:49:34.600377Z","shell.execute_reply":"2022-03-25T09:49:47.205288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\npreds = clf.predict(features_test)\naccuracy_score(labels_test, preds)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T09:49:47.209892Z","iopub.execute_input":"2022-03-25T09:49:47.210154Z","iopub.status.idle":"2022-03-25T09:49:48.264845Z","shell.execute_reply.started":"2022-03-25T09:49:47.210118Z","shell.execute_reply":"2022-03-25T09:49:48.264083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_path = '../input/test-image/'\nfeatures = []\nmembers = []\nfor f in listdir(dataset_path):\n    members.append(f)\n    image_path = dataset_path + f\n    image = cv2.imread(image_path)\n    image = cv2.resize(image, (224, 224), interpolation = cv2.INTER_AREA)\n    image = np.asarray(image, dtype=np.float64)\n    image = preprocess_input(image)\n    img_features = model.get_features(np.expand_dims(image, axis=0))\n    features.append(img_features[0].numpy())\n    ","metadata":{"execution":{"iopub.status.busy":"2022-03-25T09:54:47.359076Z","iopub.execute_input":"2022-03-25T09:54:47.359902Z","iopub.status.idle":"2022-03-25T09:54:47.394676Z","shell.execute_reply.started":"2022-03-25T09:54:47.359852Z","shell.execute_reply":"2022-03-25T09:54:47.393944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = clf.predict_proba(features)\nfor pred, f in zip(preds, members):\n    spred = sorted(pred, reverse=True)\n    print(f)\n    print(le.inverse_transform([pred.tolist().index(spred[i]) for i in range(5)]))\n    print(spred[:5])\n    print()","metadata":{"execution":{"iopub.status.busy":"2022-03-25T09:54:49.591378Z","iopub.execute_input":"2022-03-25T09:54:49.591821Z","iopub.status.idle":"2022-03-25T09:54:49.604803Z","shell.execute_reply.started":"2022-03-25T09:54:49.591786Z","shell.execute_reply":"2022-03-25T09:54:49.603984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}